{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prysie/Orange-Neural-Net-Classifier/blob/master/PBT_for_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Surrogate"
      ],
      "metadata": {
        "id": "2b-k6SBlE0oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt9CQ2DJ1vS-",
        "outputId": "94502a50-01f3-4f1e-a1fe-c5a128610403"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Required for the parrellism"
      ],
      "metadata": {
        "id": "rNNSaDd3MpLC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"CUDA is not available. Running on CPU.\")"
      ],
      "metadata": {
        "id": "idbKyDfaf0k5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import multiprocessing as mp\n",
        "mp.set_start_method('spawn', force=True)\n",
        "import torch.multiprocessing as torch_mp"
      ],
      "metadata": {
        "id": "XjuERnWKMoVX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the training file exists train the surrogate Random Forest for usage in the GAN"
      ],
      "metadata": {
        "id": "dpdoqFT3E56i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    'learning_rate_g': [1e-4, 2e-4, 5e-4],\n",
        "    'learning_rate_d': [1e-4, 2e-4, 5e-4],\n",
        "    'beta1': [0.5, 0.7, 0.9],\n",
        "    'beta2': [0.9, 0.99, 0.999],\n",
        "    'batch_size': [8, 16, 32, 64],\n",
        "    'replay_buffer_size': [10000, 50000, 100000],\n",
        "    'n_critic': [1, 5, 10],\n",
        "    'gradient_penalty_weight': [5, 10, 20]\n",
        "}"
      ],
      "metadata": {
        "id": "mdtd9ACKPKqI"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def save_training_data(training_data, base_filename='gan_training_data'):\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except DriveAlreadyMountedException:\n",
        "        print(\"Google Drive is already mounted. Proceeding without remounting.\")\n",
        "\n",
        "    # Create the directory path on Google Drive\n",
        "    drive_path = '/content/drive/MyDrive/PBT/Datasets/GAN'\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    # Generate a unique filename with a timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"{base_filename}_{timestamp}.csv\"\n",
        "\n",
        "    # Save the data to Google Drive\n",
        "    file_path = os.path.join(drive_path, filename)\n",
        "    training_data.to_csv(file_path, index=False)\n",
        "    print(f\"Data saved to {file_path}\")"
      ],
      "metadata": {
        "id": "7LQIrTXjHdpA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_latest_training_data(save_dir='/content/drive/MyDrive/PBT/Datasets/GAN'):\n",
        "    # Mount Google Drive\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "    except:\n",
        "        print(\"Google Drive is already mounted. Proceeding without remounting.\")\n",
        "\n",
        "    # Get the list of CSV files in the save directory\n",
        "    csv_files = [f for f in os.listdir(save_dir) if f.endswith('.csv')]\n",
        "\n",
        "    # Sort the files by modification time to get the latest\n",
        "    latest_file = sorted(csv_files, key=lambda x: os.path.getmtime(os.path.join(save_dir, x)), reverse=True)[0]\n",
        "\n",
        "    # Load the latest file into a DataFrame\n",
        "    latest_file_path = os.path.join(save_dir, latest_file)\n",
        "    training_data = pd.read_csv(latest_file_path)\n",
        "\n",
        "    print(f\"Loaded training data from {latest_file_path}\")\n",
        "    return training_data"
      ],
      "metadata": {
        "id": "U5MIDDMDGjf5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYYy2OB-Sh6C"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Random Forest Regressor is being trained to predict the Inception Score, a widely used metric to evaluate the quality of generated images from Generative Adversarial Networks (GANs)."
      ],
      "metadata": {
        "id": "pxUowy3OSig2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def train_surrogate_model(training_data_file):\n",
        "    # Check if the training data file exists and is not empty\n",
        "    if not os.path.exists(training_data_file) or os.stat(training_data_file).st_size == 0:\n",
        "        print(f\"Training data file '{training_data_file}' does not exist or is empty. Skipping surrogate model training.\")\n",
        "        return None\n",
        "\n",
        "    # Load the training data from the file\n",
        "    training_data = load_latest_training_data(training_data_file)\n",
        "\n",
        "    # Check if the DataFrame is empty\n",
        "    if training_data.empty:\n",
        "        print(\"Loaded training data is empty. Skipping surrogate model training.\")\n",
        "        return None\n",
        "\n",
        "    # Extract the hyperparameters and the Inception Score\n",
        "    X = training_data[['learning_rate_d', 'learning_rate_g', 'beta1', 'beta2', 'latent_dim', 'batch_size', 'replay_buffer_size', 'n_critic', 'gradient_penalty_weight']]\n",
        "    y = training_data['inception_score']\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train the Random Forest Regressor\n",
        "    rf = RandomForestRegressor(n_estimators=100)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Surrogate model trained successfully.\")\n",
        "    return rf\n",
        "\n",
        "# Example usage\n",
        "training_data_file = '/content/drive/MyDrive/PBT/Datasets/GAN/training_data.csv'\n",
        "surrogate_model = train_surrogate_model(training_data_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k_XUNIuEtRv",
        "outputId": "9b5096d3-d99b-468b-ed2d-7ed3cd77fc54"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data file '/content/drive/MyDrive/PBT/Datasets/GAN/training_data.csv' does not exist or is empty. Skipping surrogate model training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ4TaNh44QzO"
      },
      "source": [
        "# GAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JysLVI3Atl_8",
        "outputId": "a6f15f31-fc4f-4a90-bf30-64c27f9a70ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Dataset size: 50000\n",
            "Batch shape: torch.Size([128, 3, 64, 64])\n",
            "Single image shape: torch.Size([3, 64, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the data loader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Resize real images to match Generator output\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2471, 0.2435, 0.2616])\n",
        "])\n",
        "\n",
        "dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "data_loader = DataLoader(dataset, batch_size=128, shuffle=True, num_workers=4)\n",
        "\n",
        "# Print dataset size\n",
        "print(\"Dataset size:\", len(dataset))\n",
        "\n",
        "# Print the shape of data samples\n",
        "data_iter = iter(data_loader)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "print(\"Batch shape:\", images.shape)\n",
        "print(\"Single image shape:\", images[0].shape)\n",
        "\n",
        "# Load the pretrained ResNet18 model\n",
        "cifar_classifier = models.resnet18(pretrained=True)\n",
        "\n",
        "# Adapt the classifier for CIFAR-10\n",
        "num_ftrs = cifar_classifier.fc.in_features\n",
        "cifar_classifier.fc = torch.nn.Linear(num_ftrs, 10)  # CIFAR-10 has 10 classes\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cifar_classifier = cifar_classifier.to(device)\n",
        "cifar_classifier.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbF1Vr6u_S1z"
      },
      "source": [
        "GAN Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mOsbmCea_JaJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import Resize\n",
        "resize = Resize((64, 64))\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=100, num_channels=3):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(64, num_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        #print(f\"Generator input z shape: {z.shape}\")\n",
        "        z = z.view(z.size(0), self.latent_dim, 1, 1)\n",
        "        output = self.model(z)\n",
        "        #print(f\"Generator output shape: {output.shape}\")\n",
        "        return output\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, num_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.num_channels = num_channels\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, 64, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(512, 1, 4, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #print(f\"Discriminator input x shape: {x.shape}\")\n",
        "        output = self.model(x)\n",
        "        output = output.view(-1, 1).squeeze(1)\n",
        "        #print(f\"Discriminator output shape: {output.shape}\")\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP5GGnwl_U_p"
      },
      "source": [
        "Training Loop with WGAN-GP Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wkgKhIE_3S4"
      },
      "source": [
        "Step Each step consists of K = 5 gradient descent updates of the discriminator followed by a single update\n",
        "of the generator using the Adam optimiser (Kingma & Ba, 2015). We train using the WGAN-GP (Gulrajani\n",
        "11\n",
        "et al., 2017) objective where the discriminator estimates the Wasserstein distance between the real and generated data distributions, with the Lipschitz constraint enforced by regularising its input gradient to have a unit\n",
        "norm. See Appendix A.3 for additional details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "silOmXEZ_OnV"
      },
      "outputs": [],
      "source": [
        "def compute_gradient_penalty(discriminator, real_data, fake_data, device, gradient_penalty_weight):\n",
        "    \"\"\" Compute the gradient penalty for the WGAN-GP. \"\"\"\n",
        "    alpha = torch.rand(real_data.size(0), 1, 1, 1, device=device)\n",
        "    alpha = alpha.expand_as(real_data)\n",
        "\n",
        "    interpolated = alpha * real_data.data + (1 - alpha) * fake_data.data\n",
        "    interpolated.requires_grad = True\n",
        "\n",
        "    prob_interpolated = discriminator(interpolated)\n",
        "\n",
        "    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                                    grad_outputs=torch.ones(prob_interpolated.size()).to(device),\n",
        "                                    create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * gradient_penalty_weight\n",
        "    #print(f\"Gradient tensor shape: {gradients.shape}\")\n",
        "    return gradient_penalty\n",
        "\n",
        "def train_step(generator, discriminator, gen_optimizer, disc_optimizer, real_data, batch_size, device, epoch):\n",
        "  start_time = time.time()\n",
        "  real_data = resize(real_data).to(device)\n",
        "  # Multiple discriminator updates\n",
        "  for _ in range(5):\n",
        "      start_disc_time = time.time()\n",
        "      # Sample noise as generator input\n",
        "      z = torch.randn(batch_size, 100, device=device)\n",
        "      fake_data = generator(z).detach()\n",
        "\n",
        "      # Reset gradients\n",
        "      disc_optimizer.zero_grad()\n",
        "\n",
        "      # Discriminator loss\n",
        "      real_loss = discriminator(real_data)\n",
        "      fake_loss = discriminator(fake_data)\n",
        "      disc_loss = -(torch.mean(real_loss) - torch.mean(fake_loss))\n",
        "\n",
        "      # WGAN-GP gradient penalty\n",
        "      gradient_penalty = compute_gradient_penalty(discriminator, real_data, fake_data, device)\n",
        "      disc_loss += gradient_penalty\n",
        "\n",
        "      # Backprop and optimize\n",
        "      disc_loss.backward()\n",
        "      disc_optimizer.step()\n",
        "      end_disc_time = time.time()\n",
        "      logging.info(f\"Completed discriminator update {i+1}: took {end_disc_time - start_disc_time} seconds\")\n",
        "\n",
        "  # Generator update\n",
        "  logging.info(\"Starting generator update\")\n",
        "  start_gen_time = time.time()\n",
        "  gen_optimizer.zero_grad()\n",
        "  z = torch.randn(batch_size, 100, device=device)\n",
        "  fake_data = generator(z)\n",
        "  gen_loss = -torch.mean(discriminator(fake_data))\n",
        "  gen_loss.backward()\n",
        "  gen_optimizer.step()\n",
        "  end_gen_time = time.time()\n",
        "  logging.info(f\"Completed generator update: took {end_gen_time - start_gen_time} seconds\")\n",
        "\n",
        "  # Log the losses and hyperparameters\n",
        "  learning_rate_d = disc_optimizer.param_groups[0]['lr']\n",
        "  learning_rate_g = gen_optimizer.param_groups[0]['lr']\n",
        "  d_loss_value = d_loss.item()  # Assuming d_loss is calculated in your training code\n",
        "  g_loss_value = g_loss.item()  # Assuming g_loss is calculated in your training code\n",
        "\n",
        "  # Append to the DataFrame\n",
        "  training_data.loc[len(training_data)] = [epoch, d_loss_value, g_loss_value, learning_rate_d, learning_rate_g]\n",
        "  end_time = time.time()\n",
        "  logging.info(f\"Finished train_step at {end_time}, duration: {end_time - start_time} seconds\")\n",
        "\n",
        "  # Return losses if needed\n",
        "  return d_loss.item(), g_loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL1v_XkcAYgS"
      },
      "source": [
        "computed from the outputs of a pretrained CIFAR classifier (as used in Rosca et al. (2017)) rather than an\n",
        "ImageNet classifier, to avoid directly optimising the final performance metric. The CIFAR Inception scorer\n",
        "uses a much smaller network, making evaluation much faster\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cPJhDRcFAY2e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn.functional import softmax\n",
        "from scipy.stats import entropy\n",
        "\n",
        "torch.set_num_threads(2)\n",
        "\n",
        "def inception_score(imgs, cifar_classifier, cuda=False, batch_size=32, splits=1):\n",
        "  start_time = time.time()\n",
        "  \"\"\"Compute the Inception Score for generated images using CIFAR classifier.\"\"\"\n",
        "  N = len(imgs)\n",
        "  dataloader = torch.utils.data.DataLoader(imgs, batch_size=batch_size)\n",
        "\n",
        "  # Move classifier to GPU if using CUDA\n",
        "  if cuda:\n",
        "      cifar_classifier.cuda()\n",
        "\n",
        "  preds = []\n",
        "  with torch.no_grad():\n",
        "      for batch in dataloader:\n",
        "          if cuda:\n",
        "              batch = batch.cuda()\n",
        "          pred = cifar_classifier(batch)\n",
        "          preds.append(softmax(pred, dim=1).cpu().numpy())\n",
        "\n",
        "  # Now compute the mean kl-div\n",
        "  preds = np.concatenate(preds, 0)\n",
        "  split_scores = []\n",
        "\n",
        "  for k in range(splits):\n",
        "      part = preds[k * (N // splits): (k+1) * (N // splits), :]\n",
        "      py = np.mean(part, axis=0)\n",
        "      scores = []\n",
        "      for i in range(part.shape[0]):\n",
        "          pyx = part[i, :]\n",
        "          scores.append(entropy(pyx, py))\n",
        "      split_scores.append(np.exp(np.mean(scores)))\n",
        "  end_time = time.time()  # Add this line to record the end time\n",
        "  print(f\"Worker {self.id}: Inception score time: {end_time - start_time:.2f} seconds\")\n",
        "  return np.mean(split_scores), np.std(split_scores)\n",
        "\n",
        "# Example usage:\n",
        "# imgs is a batch of generated images from your GAN\n",
        "# cifar_classifier is your pretrained CIFAR classifier\n",
        "# inception_score_value, std_dev = inception_score(imgs, cifar_classifier, cuda=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrM-dtTA_bp-"
      },
      "source": [
        "PBT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This guy does all the work - running in a multiprocessor framework for parrellism"
      ],
      "metadata": {
        "id": "cfpCO7KjRAKE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "18Zr34WE_dM2"
      },
      "outputs": [],
      "source": [
        "import uuid\n",
        "import torch.multiprocessing as torch_mp\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "class Worker(torch_mp.Process):\n",
        "  def __init__(self, generator, discriminator, gen_lr, disc_lr, beta1, beta2, cifar_classifier, device, training_data, use_surrogate=False):\n",
        "      super(Worker, self).__init__()\n",
        "      self.id = uuid.uuid4()\n",
        "      self.generator = generator\n",
        "      self.discriminator = discriminator\n",
        "      self.gen_optimizer = optim.Adam(generator.parameters(), lr=gen_lr, betas=(beta1, beta2))\n",
        "      self.disc_optimizer = optim.Adam(discriminator.parameters(), lr=disc_lr, betas=(beta1, beta2))\n",
        "      self.cifar_classifier = cifar_classifier\n",
        "      self.device = device\n",
        "      self.inception_score = 0\n",
        "      self.training_data = training_data\n",
        "      self.use_surrogate = use_surrogate\n",
        "      self.latent_dim = 100\n",
        "      self.batch_size = hyperparameters['batch_size'][0]\n",
        "      print(f\"Worker {self.id}: Batch size: {self.batch_size}\")\n",
        "      self.replay_buffer_size = hyperparameters['replay_buffer_size'][0]\n",
        "      self.n_critic = hyperparameters['n_critic'][0]\n",
        "      self.gradient_penalty_weight = hyperparameters['gradient_penalty_weight'][0]\n",
        "      self.gradient_penalty_weight = hyperparameters['gradient_penalty_weight'][0]\n",
        "\n",
        "  def run(self, data_loader, steps):\n",
        "      self.train(data_loader, steps)\n",
        "      self.inception_score = self.evaluate()\n",
        "      return self.inception_score\n",
        "\n",
        "  def set_batch_size(self, batch_size):\n",
        "      self.batch_size = batch_size\n",
        "      return self.batch_size\n",
        "\n",
        "  def set_replay_buffer_size(self, replay_buffer_size):\n",
        "      self.replay_buffer_size = replay_buffer_size\n",
        "      return self.replay_buffer_size\n",
        "\n",
        "  def set_n_critic(self, n_critic):\n",
        "      self.n_critic = n_critic\n",
        "      return self.n_critic\n",
        "\n",
        "  def set_gradient_penalty_weight(self, gradient_penalty_weight):\n",
        "      self.gradient_penalty_weight = gradient_penalty_weight\n",
        "      return self.gradient_penalty_weight\n",
        "\n",
        "  def get_inception_score(self):\n",
        "      return self.inception_score\n",
        "\n",
        "\n",
        "  def train(self, data_loader, steps):\n",
        "    start_time = time.time()\n",
        "    for step in range(steps):\n",
        "      for _ in range(5):\n",
        "        real_data, _ = next(iter(data_loader))\n",
        "        real_data = real_data.to(self.device)\n",
        "        batch_size = real_data.size(0)\n",
        "\n",
        "        self.disc_optimizer.zero_grad()\n",
        "\n",
        "        z = torch.randn(batch_size, self.latent_dim, 1, 1).to(self.device)\n",
        "        fake_data = self.generator(z)\n",
        "\n",
        "        real_loss = self.discriminator(real_data).mean()\n",
        "        fake_loss = self.discriminator(fake_data.detach()).mean()\n",
        "        disc_loss = fake_loss - real_loss\n",
        "\n",
        "        gp = compute_gradient_penalty(self.discriminator, real_data, fake_data, self.device, self.gradient_penalty_weight)\n",
        "        disc_loss += gp\n",
        "        disc_loss.backward()\n",
        "        self.disc_optimizer.step()\n",
        "\n",
        "        self.gen_optimizer.zero_grad()\n",
        "\n",
        "        z = torch.randn(batch_size, self.latent_dim, device=self.device)\n",
        "        fake_data = self.generator(z)\n",
        "        gen_loss = -self.discriminator(fake_data).mean()\n",
        "        gen_loss.backward()\n",
        "        self.gen_optimizer.step()\n",
        "        torch.cuda.empty_cache()\n",
        "        self.log_data(step, gen_loss.item(), disc_loss.item(), real_loss.item(), fake_loss.item())\n",
        "\n",
        "    end_time = time.time()  # Add this line to record the end time\n",
        "    print(f\"Worker {self.id}: Training time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "  def log_data(self, step, gen_loss, disc_loss, real_loss, fake_loss):\n",
        "      new_row = pd.DataFrame({\n",
        "          'step': [step],\n",
        "          'gen_loss': [gen_loss],\n",
        "          'disc_loss': [disc_loss],\n",
        "          'real_loss': [real_loss],\n",
        "          'fake_loss': [fake_loss],\n",
        "          'learning_rate_g': [self.gen_optimizer.param_groups[0]['lr']],\n",
        "          'learning_rate_d': [self.disc_optimizer.param_groups[0]['lr']],\n",
        "          'beta1': [self.gen_optimizer.param_groups[0]['betas'][0]],\n",
        "          'beta2': [self.gen_optimizer.param_groups[0]['betas'][1]],\n",
        "          'batch_size': [self.batch_size],\n",
        "          'latent_dim': [self.latent_dim],\n",
        "          'replay_buffer_size': [self.replay_buffer_size],\n",
        "          'n_critic': [self.n_critic],\n",
        "          'gradient_penalty_weight': [self.gradient_penalty_weight]\n",
        "      })\n",
        "      self.training_data = pd.concat([self.training_data, new_row], ignore_index=True)\n",
        "\n",
        "  def evaluate(self, num_samples=50000, surrogate_threshold=7.0):\n",
        "    start_time = time.time()\n",
        "    if not self.use_surrogate:\n",
        "      self.generator.eval()\n",
        "      z = torch.randn(num_samples, 100, 1, 1).to(self.device)\n",
        "      with torch.no_grad():\n",
        "          fake_data = self.generator(z)\n",
        "      fake_data = fake_data.cpu()\n",
        "      self.inception_score, _ = inception_score(fake_data.cpu(), self.cifar_classifier.cpu(), cuda=False, batch_size=32, splits=10)\n",
        "      self.generator.train()\n",
        "      end_time = time.time()\n",
        "      print(f\"Worker {self.id}: Evaluation time: {end_time - start_time:.2f} seconds\")\n",
        "      return self.inception_score\n",
        "\n",
        "  def exploit(self, population):\n",
        "    start_time = time.time()\n",
        "    print(\"Exploiting better hyperparameters...\")\n",
        "\n",
        "    # Determine selection method and select a better worker based on the Inception score\n",
        "    if np.random.random() < 0.5:\n",
        "        # Truncation selection\n",
        "        print(\"Using truncation selection...\")\n",
        "        idx = np.argsort([w.inception_score for w in population])[-int(len(population) * 0.2):]\n",
        "        better_worker = population[np.random.choice(idx)]\n",
        "    else:\n",
        "        # Binary tournament selection\n",
        "        print(\"Using binary tournament selection...\")\n",
        "        idx = np.random.choice(len(population), size=2, replace=False)\n",
        "        better_worker = population[idx[0]] if population[idx[0]].inception_score > population[idx[1]].inception_score else population[idx[1]]\n",
        "\n",
        "    # Print out the old and new learning rates and hyperparameters\n",
        "    print(f\"Old Generator LR: {self.gen_optimizer.param_groups[0]['lr']}, New Generator LR: {better_worker.gen_optimizer.param_groups[0]['lr']}\")\n",
        "    print(f\"Old Discriminator LR: {self.disc_optimizer.param_groups[0]['lr']}, New Discriminator LR: {better_worker.disc_optimizer.param_groups[0]['lr']}\")\n",
        "\n",
        "    # Load the state dict from better_worker to current worker\n",
        "    self.generator.load_state_dict(better_worker.generator.state_dict())\n",
        "    self.discriminator.load_state_dict(better_worker.discriminator.state_dict())\n",
        "    self.gen_optimizer.load_state_dict(better_worker.gen_optimizer.state_dict())\n",
        "    self.disc_optimizer.load_state_dict(better_worker.disc_optimizer.state_dict())\n",
        "    print(f\"Worker {self.id}: Exploit time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "    def explore(self):\n",
        "      start_time = time.time()\n",
        "      # Print initial hyperparameters\n",
        "      print(\"Exploring new hyperparameters...\")\n",
        "      print(f\"Initial learning rates - Generator: {self.gen_optimizer.param_groups[0]['lr']}, Discriminator: {self.disc_optimizer.param_groups[0]['lr']}\")\n",
        "      print(f\"Initial batch size: {self.batch_size}, Initial latent dimension: {self.latent_dim}\")\n",
        "\n",
        "      # Update learning rates\n",
        "      self.gen_optimizer.param_groups[0]['lr'] *= np.random.uniform(0.5, 2.0)\n",
        "      self.disc_optimizer.param_groups[0]['lr'] *= np.random.uniform(0.5, 2.0)\n",
        "\n",
        "      # Update batch size and latent dimension\n",
        "      old_batch_size = self.batch_size\n",
        "      old_latent_dim = self.latent_dim\n",
        "      self.batch_size = np.random.choice(hyperparameters['batch_size'])\n",
        "      self.latent_dim = np.random.choice(hyperparameters['latent_dim'])\n",
        "\n",
        "      # Print updated hyperparameters\n",
        "      print(f\"Updated learning rates - Generator: {self.gen_optimizer.param_groups[0]['lr']}, Discriminator: {self.disc_optimizer.param_groups[0]['lr']}\")\n",
        "      print(f\"Updated batch size from {old_batch_size} to {self.batch_size}\")\n",
        "      print(f\"Updated latent dimension from {old_latent_dim} to {self.latent_dim}\")\n",
        "      print(f\"Worker {self.id}: Explore time: {end_time - start_time:.2f} seconds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper class to parallise the Worker"
      ],
      "metadata": {
        "id": "7gmWdf_PUL0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def worker_fn(worker, data_loader, steps, queue):\n",
        "    inception_score = worker.run(data_loader, steps)\n",
        "    queue.put((worker.id, inception_score))"
      ],
      "metadata": {
        "id": "U3mJp9CgULMb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z164haxL_foP"
      },
      "source": [
        "Run the experiement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "g4hjQELN_iOb"
      },
      "outputs": [],
      "source": [
        "def run_pbt(population_size, generations, data_loader, cifar_classifier, device, training_data, use_surrogate=False, debug_mode=False, use_pbt=True):\n",
        "    population = []\n",
        "    for _ in range(population_size):\n",
        "        # Sample hyperparameters from the predefined ranges\n",
        "        gen_lr = np.random.choice(hyperparameters['learning_rate_g'])\n",
        "        disc_lr = np.random.choice(hyperparameters['learning_rate_d'])\n",
        "        beta1 = np.random.choice(hyperparameters['beta1'])\n",
        "        beta2 = np.random.choice(hyperparameters['beta2'])\n",
        "        batch_size = np.random.choice(hyperparameters['batch_size'])\n",
        "        replay_buffer_size = np.random.choice(hyperparameters['replay_buffer_size'])\n",
        "        n_critic = np.random.choice(hyperparameters['n_critic'])\n",
        "        gradient_penalty_weight = np.random.choice(hyperparameters['gradient_penalty_weight'])\n",
        "\n",
        "        # Create a new worker with the sampled hyperparameters\n",
        "        worker = Worker(Generator().to(device),\n",
        "                        Discriminator().to(device),\n",
        "                        gen_lr, disc_lr, beta1, beta2,\n",
        "                        cifar_classifier, device, training_data, use_surrogate=use_surrogate)\n",
        "        worker.set_batch_size(batch_size)\n",
        "        worker.set_replay_buffer_size(replay_buffer_size)\n",
        "        worker.set_n_critic(n_critic)\n",
        "        worker.set_gradient_penalty_weight(gradient_penalty_weight)\n",
        "        population.append(worker)\n",
        "\n",
        "    all_inception_scores = []\n",
        "    best_inception_scores = []\n",
        "    mean_inception_scores = []\n",
        "    best_models = []\n",
        "\n",
        "    if debug_mode:\n",
        "        training_dataset, _ = torch.utils.data.random_split(dataset, [50, len(dataset) - 50])\n",
        "        training_dataloader = DataLoader(training_dataset, batch_size=12, shuffle=True, num_workers=2)\n",
        "        train_steps = 5\n",
        "        print(f\"Debug mode: Training dataset size: {len(training_dataset)}\")\n",
        "        print(f\"Debug mode: Training steps: {train_steps}\")\n",
        "    else:\n",
        "        training_dataloader = data_loader\n",
        "        train_steps = 5000\n",
        "\n",
        "    for generation in range(generations):\n",
        "        print(f\"Generation {generation + 1}\")\n",
        "\n",
        "        queue = mp.Queue()\n",
        "        processes = []\n",
        "\n",
        "        if debug_mode:\n",
        "            training_dataset, _ = torch.utils.data.random_split(dataset, [50, len(dataset) - 50])\n",
        "            training_dataloader = DataLoader(training_dataset, batch_size=12, shuffle=True, num_workers=2)\n",
        "            train_steps = 5\n",
        "        else:\n",
        "            training_dataloader = data_loader\n",
        "            train_steps = 5000\n",
        "\n",
        "        for worker in population:\n",
        "            worker.start()\n",
        "            processes.append(worker)\n",
        "\n",
        "        for worker in processes:\n",
        "            inception_score = worker.run(training_dataloader, train_steps)\n",
        "            queue.put(inception_score)\n",
        "\n",
        "        # Collect the results\n",
        "        inception_scores = [queue.get() for _ in range(len(population))]\n",
        "        inception_scores.sort(key=lambda x: x[0])  # Sort by worker ID\n",
        "        inception_scores = [score for _, score in inception_scores]\n",
        "\n",
        "        for worker_idx, inception_score in enumerate(inception_scores):\n",
        "            print(f\"Worker {worker_idx + 1} Inception Score: {inception_score:.4f}\")\n",
        "            all_inception_scores.append(inception_score)\n",
        "\n",
        "        if use_pbt:\n",
        "          # Record best and mean Inception scores for the current generation\n",
        "          best_inception_score = max(worker.inception_score for worker in population)\n",
        "          mean_inception_score = sum(worker.inception_score for worker in population) / len(population)\n",
        "          best_inception_scores.append(best_inception_score)\n",
        "          mean_inception_scores.append(mean_inception_score)\n",
        "\n",
        "          # Record the best model for the current generation\n",
        "          best_model_idx = np.argmax([worker.inception_score for worker in population])\n",
        "          best_models.append(population[best_model_idx].generator.state_dict())\n",
        "\n",
        "          # Exploit and explore\n",
        "          for worker in population:\n",
        "            if np.random.random() < 0.5:\n",
        "              worker.exploit(population)\n",
        "            else:\n",
        "              worker.explore()\n",
        "\n",
        "    if use_pbt:\n",
        "      # Visualize generated samples from the best models\n",
        "      best_model = Generator().to(device)\n",
        "      for i, model_state_dict in enumerate(best_models):\n",
        "        best_model.load_state_dict(model_state_dict)\n",
        "        z = torch.randn(16, 100, 1, 1).to(device)\n",
        "        samples = best_model(z)\n",
        "        samples = (samples + 1) / 2  # Rescale to [0, 1]\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        for j in range(16):\n",
        "            plt.subplot(4, 4, j + 1)\n",
        "            plt.imshow(samples[j].permute(1, 2, 0).cpu().numpy())\n",
        "            plt.axis('off')\n",
        "        plt.title(f\"Best Model at Generation {i + 1}\")\n",
        "        plt.show()\n",
        "\n",
        "    return population"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "GT61jaTQtbsv",
        "outputId": "e027bb3a-61ce-4320-c9e2-9dd9084cbf1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker cdbf4ff2-e580-40b2-9a9e-1910eed4457c: Batch size: 8\n",
            "Worker 22760dcb-61d5-40e7-b8c3-01793242e758: Batch size: 8\n",
            "Debug mode: Training dataset size: 50\n",
            "Debug mode: Training steps: 5\n",
            "Generation 1\n",
            "Worker cdbf4ff2-e580-40b2-9a9e-1910eed4457c: Training time: 225.97 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 6.10 GiB. GPU 0 has a total capacity of 22.17 GiB of which 3.20 GiB is free. Process 4423 has 18.96 GiB memory in use. Of the allocated memory 18.68 GiB is allocated by PyTorch, and 45.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-58131051b2fc>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Run without the surrogate model to collect training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfinal_population\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_pbt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcifar_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_surrogate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0msave_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-c9a4bcaa51c5>\u001b[0m in \u001b[0;36mrun_pbt\u001b[0;34m(population_size, generations, data_loader, cifar_classifier, device, training_data, use_surrogate, debug_mode, use_pbt)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mworker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0minception_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minception_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-edc2b2e80224>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_loader, steps)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-edc2b2e80224>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, num_samples, surrogate_threshold)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m           \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m       \u001b[0mfake_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minception_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minception_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcifar_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-34f9d251ce11>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m#print(f\"Generator input z shape: {z.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(f\"Generator output shape: {output.shape}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    176\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2482\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.10 GiB. GPU 0 has a total capacity of 22.17 GiB of which 3.20 GiB is free. Process 4423 has 18.96 GiB memory in use. Of the allocated memory 18.68 GiB is allocated by PyTorch, and 45.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "#if __name__ == '__main__':\n",
        "#  mp.set_start_method('spawn')  # Set the start method for multiprocessing\n",
        "\n",
        "training_data = pd.DataFrame(columns=['step', 'gen_loss', 'disc_loss', 'real_loss', 'fake_loss',\n",
        "                                      'learning_rate_g', 'learning_rate_d', 'beta1', 'beta2',\n",
        "                                      'batch_size', 'latent_dim', 'replay_buffer_size', 'n_critic',\n",
        "                                      'gradient_penalty_weight'])\n",
        "\n",
        "population_size = 2\n",
        "generations = 1\n",
        "#population_size = 45\n",
        "#generations = 400\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Run without the surrogate model to collect training data\n",
        "final_population = run_pbt(population_size, generations, data_loader, cifar_classifier, device, training_data, debug_mode=True, use_surrogate=False)\n",
        "save_training_data(training_data)\n",
        "\n",
        "final_population = run_pbt(population_size, generations, data_loader, cifar_classifier, device, training_data, debug_mode=True, use_surrogate=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation of Results"
      ],
      "metadata": {
        "id": "d_TwD-qsWjuh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(training_data.shape())"
      ],
      "metadata": {
        "id": "hz9pPe2j5c1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('gan_training_data.csv')\n",
        "\n",
        "# Plot learning curves\n",
        "plt.figure(figsize=(12, 6))\n",
        "for key in ['Real_Loss', 'Fake_Loss', 'Gen_Loss']:\n",
        "    plt.plot(data[key], label=key)\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Inception Scores\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data['Inception_Score'], label='Inception Score')\n",
        "plt.xlabel('Training Steps')\n",
        "plt.ylabel('Inception Score')\n",
        "plt.title('Inception Score Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ke3A6xELWjSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GAN Random Forest Surrogate Model"
      ],
      "metadata": {
        "id": "tNR0IJhxEm6O"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOfmiVfX9y75Px9kDVVNtiy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}